# -*- coding: utf-8 -*-
"""pazesh(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mNB9P4IljXBcRS7EG4P52d-UlZD5EfkN
"""

import numpy as np
import tensorflow as tf
import pandas as pd
from collections import Counter
import csv, re, pickle

fear = pd.read_excel("/content/qq.xlsx")

fear =fear.dropna()
from sklearn.model_selection import train_test_split
train, test = train_test_split(fear, test_size=0.20, random_state=18)



from sklearn.feature_extraction.text import CountVectorizer
from sklearn.base import TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS
from sklearn.metrics import accuracy_score
from nltk.corpus import stopwords
import string
import re
import spacy
spacy.load('en')
from spacy.lang.en import English
parser = English()

import nltk
nltk.download('stopwords')

SYMBOLS = " ".join(string.punctuation).split(" ") + ["-", "...", "”", "”"]
class CleanTextTransformer(TransformerMixin):   
  def transform(self, X, **transform_params):
        return [cleanText(text) for text in X]   
  def fit(self, X, y=None, **fit_params):
        return self
def get_params(self, deep=True):
        return {}
    
def cleanText(text):
    text = text.strip().replace("\n", " ").replace("\r", " ")
    text = text.lower()
    return text
def tokenizeText(sample):
    tokens = parser(sample)
    lemmas = []
    for tok in tokens:
        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != "-PRON-" else tok.lower_)
    tokens = lemmas
    tokens = [tok for tok in tokens if tok not in STOPLIST]
    tokens = [tok for tok in tokens if tok not in SYMBOLS]
    return tokens

!pip install hazm
!pip install stopwords_guilannlp

import hazm 
from stopwords_guilannlp import *
normalizer =  hazm.Normalizer()
tokenizer = hazm.SentenceTokenizer()
tokenn=hazm.word_tokenize
STOPLIST=stopwords_output("Persian", "nar")

vectorizer = CountVectorizer(tokenizer=tokenn, ngram_range=(1,1))
#------------------------
clf = LinearSVC()

pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])# data
train1 = train['comment'].tolist()
labelsTrain1 = train['Label'].tolist()
test1 = test['comment'].tolist()
labelsTest1 = test['Label'].tolist()
# train
pipe.fit(train1, labelsTrain1)# test
preds = pipe.predict(test1)
print("accuracy:", accuracy_score(labelsTest1, preds))

pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])
transform = pipe.fit_transform(train1, labelsTrain1)
vocab = vectorizer.get_feature_names()
for i in range(len(train1)):
    s = ""
    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]
    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]
    for idx, num in zip(indexIntoVocab, numOccurences):
        s += str((vocab[idx], num))

from sklearn import metrics
print(metrics.classification_report(labelsTest1, preds, 
                                    target_names=fear['Label'].unique()))