{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pazesh(1)(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toktampazesh97/tamrin-datamining1/blob/main/pazesh(1)(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkw31q2AZFiq"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "from collections import Counter\r\n",
        "import csv, re, pickle\r\n",
        "\r\n",
        "fear = pd.read_excel(\"/content/pazeshh.xlsx\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6eFTyCGxXs8"
      },
      "source": [
        "fear =fear.dropna()\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "train, test = train_test_split(fear, test_size=0.20, random_state=18)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu0V-XOHPhq8"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9KaTxU-P99g"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.base import TransformerMixin\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import string\r\n",
        "import re\r\n",
        "import spacy\r\n",
        "spacy.load('en')\r\n",
        "from spacy.lang.en import English\r\n",
        "parser = English()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN-IlRh3Q91_",
        "outputId": "fe2ad79e-d16b-405e-f1be-c58485e36e8e"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSF40nXvQH4x"
      },
      "source": [
        "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\r\n",
        "class CleanTextTransformer(TransformerMixin):   \r\n",
        "  def transform(self, X, **transform_params):\r\n",
        "        return [cleanText(text) for text in X]   \r\n",
        "  def fit(self, X, y=None, **fit_params):\r\n",
        "        return self\r\n",
        "def get_params(self, deep=True):\r\n",
        "        return {}\r\n",
        "    \r\n",
        "def cleanText(text):\r\n",
        "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\r\n",
        "    text = text.lower()\r\n",
        "    return text\r\n",
        "def tokenizeText(sample):\r\n",
        "    tokens = parser(sample)\r\n",
        "    lemmas = []\r\n",
        "    for tok in tokens:\r\n",
        "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\r\n",
        "    tokens = lemmas\r\n",
        "    tokens = [tok for tok in tokens if tok not in STOPLIST]\r\n",
        "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\r\n",
        "    return tokens"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhDXzP4gD-Zj",
        "outputId": "143dd554-6175-40e1-b7aa-300b8cb48140"
      },
      "source": [
        "!pip install hazm\r\n",
        "!pip install stopwords_guilannlp"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.6/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n",
            "Requirement already satisfied: stopwords_guilannlp in /usr/local/lib/python3.6/dist-packages (13.2019.3.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmCAJrLKDJ7c"
      },
      "source": [
        "import hazm \r\n",
        "from stopwords_guilannlp import *\r\n",
        "normalizer =  hazm.Normalizer()\r\n",
        "tokenizer = hazm.SentenceTokenizer()\r\n",
        "tokenn=hazm.word_tokenize\r\n",
        "STOPLIST=stopwords_output(\"Persian\", \"nar\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP596blEQXt6",
        "outputId": "6a904eaf-d026-417e-c87f-e7a051b4f66e"
      },
      "source": [
        "\r\n",
        "vectorizer = CountVectorizer(tokenizer=tokenn, ngram_range=(1,1))\r\n",
        "#------------------------\r\n",
        "clf = LinearSVC()\r\n",
        "\r\n",
        "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])# data\r\n",
        "train1 = train['comment'].tolist()\r\n",
        "labelsTrain1 = train['Label'].tolist()\r\n",
        "test1 = test['comment'].tolist()\r\n",
        "labelsTest1 = test['Label'].tolist()\r\n",
        "# train\r\n",
        "pipe.fit(train1, labelsTrain1)# test\r\n",
        "preds = pipe.predict(test1)\r\n",
        "print(\"accuracy:\", accuracy_score(labelsTest1, preds))\r\n",
        "\r\n",
        "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\r\n",
        "transform = pipe.fit_transform(train1, labelsTrain1)\r\n",
        "vocab = vectorizer.get_feature_names()\r\n",
        "for i in range(len(train1)):\r\n",
        "    s = \"\"\r\n",
        "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\r\n",
        "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\r\n",
        "    for idx, num in zip(indexIntoVocab, numOccurences):\r\n",
        "        s += str((vocab[idx], num))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La-sltuNNBm3",
        "outputId": "d94f0982-c176-47a2-85d6-77008f35e00c"
      },
      "source": [
        "from sklearn import metrics\r\n",
        "print(metrics.classification_report(labelsTest1, preds, \r\n",
        "                                    target_names=fear['Label'].unique()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   U0001F49C       0.50      0.33      0.40         6\n",
            "   U0001F60D       0.40      0.48      0.43        42\n",
            "   U0001F49F       0.51      0.61      0.56        70\n",
            "   U0001F928       0.00      0.00      0.00        12\n",
            "   U0001F61F       0.55      0.62      0.58       109\n",
            "   U0001F613       0.27      0.25      0.26        36\n",
            "   U0001F912       0.14      0.06      0.08        17\n",
            "   U0001F4A8       0.59      0.71      0.65        45\n",
            "   U0001F63E       0.38      0.14      0.20        22\n",
            "   U0001F479       0.56      0.49      0.52        41\n",
            "\n",
            "    accuracy                           0.49       400\n",
            "   macro avg       0.39      0.37      0.37       400\n",
            "weighted avg       0.46      0.49      0.47       400\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}